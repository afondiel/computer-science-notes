{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QXFeOd7j-gV"
      },
      "source": [
        "# Natural Language Processing - Introduction\n",
        "\n",
        "NLP : Computer science, artificial Intelligence which deals with Human Language\n",
        "** Text mining ** : analysation of information from natural language text.\n",
        "\n",
        "## Application\n",
        "\n",
        "* Sentimental Analysis : like -> fb emojis\n",
        "* Chatbot : costumer assistance\n",
        "* Speech Recognition : voice assistance, like windows cortana\n",
        "* Machine Translation : google transla\n",
        "\n",
        "* Spell Checking\n",
        "* Information Extraction\n",
        "* Keyword Searching\n",
        "* Advertisement \n",
        "\n",
        "Components of NLP :\n",
        "* Natural Language Understanding\n",
        "* Natural Language Generation\n",
        "\n",
        "## Tecniques\n",
        "\n",
        "SYNTACTIC : \n",
        "\n",
        "* **Tokenization** : split a phrase into a small part od token\n",
        "* **Stemming** : normalize words into its base form or root form\n",
        "* **Lemmatization**: morphological analyse of the phrase (based on dictionary : meaning and synonym)\n",
        "* **POS tag** : classify a part of the speech : verb, adj, noun \n",
        "* **Named Entity recognition**: classify a group of word in a group : movie, monetary value, organizatio, location, quantities, person\n",
        "* **Chunking**: picking individual pieces of informations and grouping them into bigger Pieces\n",
        "\n",
        "SEMANTIC : \n",
        "\n",
        "* Ortographic Correction\n",
        "* Text generation\n",
        "* Automatic translation\n",
        "* ambiguity (lexical, syntactic, referential ...)\n",
        "\n",
        "## Tools\n",
        "\n",
        "* NLTK : https://www.nltk.org/\n",
        "\n",
        "\n",
        "src : \n",
        "- Intro NLP : https://www.youtube.com/watch?v=5ctbvkAMQO4\n",
        "- NLP w/ python & NLTK : https://www.youtube.com/watch?v=X2vAabgKiuM\n",
        "- FULL tuto : https://www.youtube.com/watch?v=PBzGxFxMCuA&list=PL75e0qA87dlFJiNMeKltWImhQxfFwaxvv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my7DPvkAyD8_"
      },
      "source": [
        "# NLP Analysis with NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIPBHczP1Y9t"
      },
      "source": [
        "#### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vv_rEYM1U6r",
        "outputId": "fd35646e-5d4b-4470-cd2f-51b7603e32ef"
      },
      "outputs": [],
      "source": [
        "#!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOY_G9hPiDIt"
      },
      "source": [
        "# **SYNTACTIC ANALYSIS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkioYtNcgp4Q"
      },
      "source": [
        "## **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfhqTsdJjnWR"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import nltk \n",
        "import nltk.corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcKaVu7WCN8u"
      },
      "outputs": [],
      "source": [
        "# print(os.listdir(nltk.data.find(\"corpora\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCOE4ZY7C0dA"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3vJwzkgC5Cq"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords.words()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfIzcsMgCIVj"
      },
      "source": [
        "### Brown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA1fL0Rs7QEM"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "brown.words()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoNVhcBHEHru"
      },
      "source": [
        "### Glutenberg\n",
        "\n",
        "Case : contains list of books"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcqb7xonD1e5",
        "outputId": "941db29d-e811-4096-bc4f-4d5075a96e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('gutenberg')\n",
        "# print the library data \n",
        "# nltk.corpus.gutenberg.fileids() \n",
        "#looking for specific library or book\n",
        "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
        "hamlet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUUi6rDGIdHr",
        "outputId": "39e5afa7-c31d-498f-cce7-8fd9d7f8af22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
          ]
        }
      ],
      "source": [
        "for word in hamlet[:500]:\n",
        "  print(word, sep= ' ', end=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFVWAkGgL8ez"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg_t6VNvCK3T"
      },
      "source": [
        "### Working with my own string\n",
        "\n",
        "Real world case with kikongo data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjoRzZeFMN0o",
        "outputId": "f2f17134-4766-47d8-c981-6a6297d8f90d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2 Abalahami butaka Izaki ; ye Izaki butaka Yakobi ; ye Yakobi butaka Yuda ti ba mpangi na yandi ; 3 ye Yuda butaka Falezi ti Zala, ya Tamari ; ye Falezi butaka Esloni ; ye Esloni butaka Alami ; 4 ye Alami butaka Aminadabi ; ye Aminadabi butaka Nasoni ; ye Nasoni butaka Salmoni ; 5 ye Salmoni butaka Buzi, na Lakabi ; ye Buzi butaka Obedi, na Luti ; ye Obed butaka Yese ; 6 ye Yese butaka Davidi ntotila ; ye Davidi ntotila butaka Salomoni, na yina [ya vandaka nketo] ya Uli ; 7 ye Salomoni butaka Loboami ; ye Loboami butaka Abia ; ye Abia butaka Asa ; 8 ye Asa butaka Yozafati ; ye Yozafati butaka Yolami ; ye Yolami butaka Oziasi ; ye Oziasi butaka Yoatami ; 9 ye Yoatami butaka Akazi ; ye Akazi butaka Ezekiasi ; 10 ye Ezekiasi butaka Manase ; ye Manase butaka Amoni ; ye Amoni butaka Yoziasi ; 11 ye Yoziasi butaka Yekoniasi ti ba mpangi na yandi, na ntangu ya bo natamaka na Babiloni, 12 ye na nima ya kunatama na Babiloni, Yekoniasi butaka Salatiele ; ye Salatiele butaka Zolobabele ; 13 ye Zolobabele butaka Abiudi ; ye Abiudi butaka Eliakimi ; ye Eliakimi butaka Azoli ; ye Azoli butaka Sadoki ; 14 ye Sadoki butaka Akimi ; ye Akimi butaka Eliudi ;15 ye Eliudi butaka Eleazali ; Eleazali butaka Matani ; ye Matani butaka Yakobi ;16 ye Yakobi butaka Yozefo, bakala ya Maliya, na yandi butukaka Yezu, yina ya bo ke kubinga Klistu.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "kikongo = ''' 2 Abalahami butaka Izaki ; ye Izaki butaka Yakobi ; ye Yakobi butaka Yuda ti ba mpangi na yandi ; 3 ye Yuda butaka Falezi ti Zala, ya Tamari ; ye Falezi butaka Esloni ; ye Esloni butaka Alami ; 4 ye Alami butaka Aminadabi ; ye Aminadabi butaka Nasoni ; ye Nasoni butaka Salmoni ; 5 ye Salmoni butaka Buzi, na Lakabi ; ye Buzi butaka Obedi, na Luti ; ye Obed butaka Yese ; 6 ye Yese butaka Davidi ntotila ; ye Davidi ntotila butaka Salomoni, na yina [ya vandaka nketo] ya Uli ; 7 ye Salomoni butaka Loboami ; ye Loboami butaka Abia ; ye Abia butaka Asa ; 8 ye Asa butaka Yozafati ; ye Yozafati butaka Yolami ; ye Yolami butaka Oziasi ; ye Oziasi butaka Yoatami ; 9 ye Yoatami butaka Akazi ; ye Akazi butaka Ezekiasi ; 10 ye Ezekiasi butaka Manase ; ye Manase butaka Amoni ; ye Amoni butaka Yoziasi ; 11 ye Yoziasi butaka Yekoniasi ti ba mpangi na yandi, na ntangu ya bo natamaka na Babiloni, 12 ye na nima ya kunatama na Babiloni, Yekoniasi butaka Salatiele ; ye Salatiele butaka Zolobabele ; 13 ye Zolobabele butaka Abiudi ; ye Abiudi butaka Eliakimi ; ye Eliakimi butaka Azoli ; ye Azoli butaka Sadoki ; 14 ye Sadoki butaka Akimi ; ye Akimi butaka Eliudi ;15 ye Eliudi butaka Eleazali ; Eleazali butaka Matani ; ye Matani butaka Yakobi ;16 ye Yakobi butaka Yozefo, bakala ya Maliya, na yandi butukaka Yezu, yina ya bo ke kubinga Klistu.\n",
        "'''\n",
        "type_of_text = type(kikongo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-sMGQGnM0bM",
        "outputId": "58247e50-e494-4cc6-8eb8-dd487d8fd760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['2', 'Abalahami', 'butaka', 'Izaki', 'ye', 'Izaki', 'butaka', 'Yakobi', 'ye', 'Yakobi', 'butaka', 'Yuda', 'ti', 'ba', 'mpangi', 'na', 'yandi', '3', 'ye', 'Yuda', 'butaka', 'Falezi', 'ti', 'Zala', ',', 'ya', 'Tamari', 'ye', 'Falezi', 'butaka', 'Esloni', 'ye', 'Esloni', 'butaka', 'Alami', '4', 'ye', 'Alami', 'butaka', 'Aminadabi', 'ye', 'Aminadabi', 'butaka', 'Nasoni', 'ye', 'Nasoni', 'butaka', 'Salmoni', '5', 'ye', 'Salmoni', 'butaka', 'Buzi', ',', 'na', 'Lakabi', 'ye', 'Buzi', 'butaka', 'Obedi', ',', 'na', 'Luti', 'ye', 'Obed', 'butaka', 'Yese', '6', 'ye', 'Yese', 'butaka', 'Davidi', 'ntotila', 'ye', 'Davidi', 'ntotila', 'butaka', 'Salomoni', ',', 'na', 'yina', '[', 'ya', 'vandaka', 'nketo', ']', 'ya', 'Uli', '7', 'ye', 'Salomoni', 'butaka', 'Loboami', 'ye', 'Loboami', 'butaka', 'Abia', 'ye', 'Abia', 'butaka', 'Asa', '8', 'ye', 'Asa', 'butaka', 'Yozafati', 'ye', 'Yozafati', 'butaka', 'Yolami', 'ye', 'Yolami', 'butaka', 'Oziasi', 'ye', 'Oziasi', 'butaka', 'Yoatami', '9', 'ye', 'Yoatami', 'butaka', 'Akazi', 'ye', 'Akazi', 'butaka', 'Ezekiasi', '10', 'ye', 'Ezekiasi', 'butaka', 'Manase', 'ye', 'Manase', 'butaka', 'Amoni', 'ye', 'Amoni', 'butaka', 'Yoziasi', '11', 'ye', 'Yoziasi', 'butaka', 'Yekoniasi', 'ti', 'ba', 'mpangi', 'na', 'yandi', ',', 'na', 'ntangu', 'ya', 'bo', 'natamaka', 'na', 'Babiloni', ',', '12', 'ye', 'na', 'nima', 'ya', 'kunatama', 'na', 'Babiloni', ',', 'Yekoniasi', 'butaka', 'Salatiele', 'ye', 'Salatiele', 'butaka', 'Zolobabele', '13', 'ye', 'Zolobabele', 'butaka', 'Abiudi', 'ye', 'Abiudi', 'butaka', 'Eliakimi', 'ye', 'Eliakimi', 'butaka', 'Azoli', 'ye', 'Azoli', 'butaka', 'Sadoki', '14', 'ye', 'Sadoki', 'butaka', 'Akimi', 'ye', 'Akimi', 'butaka', 'Eliudi', '15', 'ye', 'Eliudi', 'butaka', 'Eleazali', 'Eleazali', 'butaka', 'Matani', 'ye', 'Matani', 'butaka', 'Yakobi', '16', 'ye', 'Yakobi', 'butaka', 'Yozefo', ',', 'bakala', 'ya', 'Maliya', ',', 'na', 'yandi', 'butukaka', 'Yezu', ',', 'yina', 'ya', 'bo', 'ke', 'kubinga', 'Klistu', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "# Ce jeton divise un texte en une liste de phrases, \n",
        "# en utilisant un algorithme non supervisé pour construire un modèle \n",
        "# les mots d'abréviation, les collocations et les mots qui commencent les phrases. \n",
        "# Il doit être formé sur une grande collection de texte en clair dans la langue cible avant de pouvoir être utilisé\n",
        "nltk.download('punkt')\n",
        "\n",
        "# nltk.download('word_tokenize')\n",
        "\n",
        "cleaned_kikongo_text = kikongo.replace(\";\", \" \")\n",
        "kikongo_token = word_tokenize(cleaned_kikongo_text)\n",
        "\n",
        "print(kikongo_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVuiKNsqNTWA",
        "outputId": "ff40ce51-5312-4436-a810-db3f52191c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "235\n"
          ]
        }
      ],
      "source": [
        "lengt_of_sentence = len(kikongo_token)\n",
        "print(lengt_of_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tybJdIhBolcG"
      },
      "source": [
        "## FreqDist\n",
        "\n",
        "Une distribution de fréquence pour les résultats d'une expérience.\n",
        "Une distribution de fréquence enregistre le nombre de fois où chaque résultat d'une expérience s'est produit.`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLo9vVW0Q4WP"
      },
      "outputs": [],
      "source": [
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fdist = FreqDist()\n",
        "fdist['ke']\n",
        "\n",
        "for word in kikongo_token:\n",
        "  fdist[word.lower()]+=1 # count every word repetition\n",
        "fdist # print the list fdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrlqDFmbSEvB",
        "outputId": "2845a2c6-49e2-4ad7-9765-85282ec89b8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 49,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fdist['yuda']\n",
        "# fdist.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOzyQ-O6TM--",
        "outputId": "c1a690e9-5d88-4c15-921a-76f6cfd2c23e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1339"
            ]
          },
          "execution_count": 16,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nbr_words = len(kikongo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBBLLDHoTOJS"
      },
      "outputs": [],
      "source": [
        "nbr_repeated_words = len(fdist)\n",
        "print(nbr_repeated_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "dVt1uQ438xpE",
        "outputId": "54ab8d98-6bbd-4ef1-9dfa-e9b150d75468"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fdist_top10 = fdist.most_common(10)\n",
        "# x,y = zip(*fdist_top10)\n",
        "xs = [x[0] for x in fdist_top10]\n",
        "ys = [x[1] for x in fdist_top10]\n",
        "# print(ys)\n",
        "plt.scatter(xs,ys)\n",
        "\n",
        "# print(len(fdist_top10))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8zaembg_U1z",
        "outputId": "7202d7a7-9228-4300-dca2-9db6bea397c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' 2 Abalahami butaka Izaki   ye Izaki butaka Yakobi   ye Yakobi butaka Yuda ti ba mpangi na yandi   3 ye Yuda butaka Falezi ti Zala, ya Tamari   ye Falezi butaka Esloni   ye Esloni butaka Alami   4 ye Alami butaka Aminadabi   ye Aminadabi butaka Nasoni   ye Nasoni butaka Salmoni   5 ye Salmoni butaka Buzi, na Lakabi   ye Buzi butaka Obedi, na Luti   ye Obed butaka Yese   6 ye Yese butaka Davidi ntotila   ye Davidi ntotila butaka Salomoni, na yina [ya vandaka nketo] ya Uli   7 ye Salomoni butaka Loboami   ye Loboami butaka Abia   ye Abia butaka Asa   8 ye Asa butaka Yozafati   ye Yozafati butaka Yolami   ye Yolami butaka Oziasi   ye Oziasi butaka Yoatami   9 ye Yoatami butaka Akazi   ye Akazi butaka Ezekiasi   10 ye Ezekiasi butaka Manase   ye Manase butaka Amoni   ye Amoni butaka Yoziasi   11 ye Yoziasi butaka Yekoniasi ti ba mpangi na yandi, na ntangu ya bo natamaka na Babiloni, 12 ye na nima ya kunatama na Babiloni, Yekoniasi butaka Salatiele   ye Salatiele butaka Zolobabele   13 ye Zolobabele butaka Abiudi   ye Abiudi butaka Eliakimi   ye Eliakimi butaka Azoli   ye Azoli butaka Sadoki   14 ye Sadoki butaka Akimi   ye Akimi butaka Eliudi  15 ye Eliudi butaka Eleazali   Eleazali butaka Matani   ye Matani butaka Yakobi  16 ye Yakobi butaka Yozefo, bakala ya Maliya, na yandi butukaka Yezu, yina ya bo ke kubinga Klistu.\\n']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import blankline_tokenize\n",
        "\n",
        "kikongo_blank = blankline_tokenize(cleaned_kikongo_text) #number of paragraph\n",
        "print(kikongo_blank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82GiyFd8_5Z8"
      },
      "outputs": [],
      "source": [
        "kikongo_blank[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiTmPtYiAVmo"
      },
      "outputs": [],
      "source": [
        "from nltk.util import bigrams, trigrams, ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSFAyx4jBAfP"
      },
      "outputs": [],
      "source": [
        "string = 'kikongo ya leta nginga diame'\n",
        "sentence = \"Mbote ba mpangi ya ntoto\"\n",
        "quotes_tokens = nltk.word_tokenize(string)\n",
        "quotes_tokens_2 = nltk.word_tokenize(sentence)\n",
        "\n",
        "quotes_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "123qEzVTBhgR"
      },
      "outputs": [],
      "source": [
        "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
        "quotes_bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9od-wGcqB9Lq"
      },
      "outputs": [],
      "source": [
        "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
        "quotes_trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM-0Y4auCC-J"
      },
      "outputs": [],
      "source": [
        "# Ngrams!!!\n",
        "quotes_ngrams = list(nltk.ngrams(quotes_tokens, 2))\n",
        "quotes_ngrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PnLW8Llg-c3"
      },
      "source": [
        "## **Stemming**\n",
        "\n",
        "Handle suffix and prefix\n",
        "\n",
        "Limitations : \n",
        "* PorterStemmer remove only 'ing'\n",
        "* Chercher de stem pour : en, fr, pt : SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf5JeNkyhUqT"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2MAnsb4BC44Q",
        "outputId": "8d444138-ac21-4796-aadb-45381545ae08"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'love'"
            ]
          },
          "execution_count": 31,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pst.stem('loving')\n",
        "# pst.stem('aimera')  #it does n't work\n",
        "# pst.stem('amando') #it does n't work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ4KoFfjQaOa",
        "outputId": "7ee19a5a-9621-423c-c428-8ac978e4a3ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mbasi:mbasi\n",
            "nzambi:nzambi\n",
            "mbote:mbote\n",
            "beto:beto\n"
          ]
        }
      ],
      "source": [
        "words_to_stem=['give', 'giving', 'given', 'gave']\n",
        "# data to stem\n",
        "kiwords_to_stem = ['mbasi', 'nzambi', 'mbote', 'beto']\n",
        "# loop data\n",
        "for words in kiwords_to_stem:\n",
        "  print(words+ ':'+ pst.stem(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um0Moug5ReQW",
        "outputId": "9a7a1cce-348f-4826-aec0-0d98b28984b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mbasi:mbas\n",
            "nzambi:nzamb\n",
            "mbote:mbot\n",
            "beto:beto\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "# using lancaster stematization, i retrive the last element of each word\n",
        "lst = LancasterStemmer()\n",
        "for words in kiwords_to_stem:\n",
        "  print(words+ ':' + lst.stem(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU1FLG4pS4wo",
        "outputId": "c0775058-bdbe-4746-dd5d-cd8a34204a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mbasi:mbasi\n",
            "nzambi:nzambi\n",
            "mbote:mbote\n",
            "beto:beto\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "sbst = SnowballStemmer('english')\n",
        "\n",
        "for words in kiwords_to_stem:\n",
        "  print(words+ ':' +sbst.stem(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OwFSK4phWme"
      },
      "source": [
        "## **Lemmatization**\n",
        "\n",
        "Morphological analysis of the word : for that have a precise dictionary to link the word "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWm15jAjWgC5",
        "outputId": "955a4842-ec42-48ba-e097-9b9236c5e3eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem import wordnet\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aLd9pLKTcO9n",
        "outputId": "b810e055-0f2c-4ab3-a0e0-34544857b940"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'kikongo'"
            ]
          },
          "execution_count": 25,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "word_len = WordNetLemmatizer()\n",
        "\n",
        "word_len.lemmatize('kikongo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l26aXhfdejxx",
        "outputId": "68e66a40-1860-488c-d15f-aec1ba6076d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "give:give\n",
            "giving:giving\n",
            "given:given\n",
            "gave:gave\n"
          ]
        }
      ],
      "source": [
        "for words in words_to_stem:\n",
        "  print(words+ ':' +word_len.lemmatize(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMU7HrsAVlU9"
      },
      "source": [
        "Stop Words(**Les mots vides**) : Words that are not need in NLP other than nouns.\n",
        "In nlp these words have to be deleted or filtered in order to make the processing easier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnStOT9bVXZF",
        "outputId": "f1e40a19-f818-44b0-b93a-ea3ad298d592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install nltk\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnyw1V-F7guH"
      },
      "source": [
        "There is a stopwords library provided by NLTK for differents Languages. To see the available languages: https://pypi.org/project/stop-words/#available-languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Ou0e9OkO9T",
        "outputId": "e8b84ccb-0b7a-44b5-fb96-1baf91f45c0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['de',\n",
              " 'a',\n",
              " 'o',\n",
              " 'que',\n",
              " 'e',\n",
              " 'é',\n",
              " 'do',\n",
              " 'da',\n",
              " 'em',\n",
              " 'um',\n",
              " 'para',\n",
              " 'com',\n",
              " 'não',\n",
              " 'uma',\n",
              " 'os',\n",
              " 'no',\n",
              " 'se',\n",
              " 'na',\n",
              " 'por',\n",
              " 'mais',\n",
              " 'as',\n",
              " 'dos',\n",
              " 'como',\n",
              " 'mas',\n",
              " 'ao',\n",
              " 'ele',\n",
              " 'das',\n",
              " 'à',\n",
              " 'seu',\n",
              " 'sua',\n",
              " 'ou',\n",
              " 'quando',\n",
              " 'muito',\n",
              " 'nos',\n",
              " 'já',\n",
              " 'eu',\n",
              " 'também',\n",
              " 'só',\n",
              " 'pelo',\n",
              " 'pela',\n",
              " 'até',\n",
              " 'isso',\n",
              " 'ela',\n",
              " 'entre',\n",
              " 'depois',\n",
              " 'sem',\n",
              " 'mesmo',\n",
              " 'aos',\n",
              " 'seus',\n",
              " 'quem',\n",
              " 'nas',\n",
              " 'me',\n",
              " 'esse',\n",
              " 'eles',\n",
              " 'você',\n",
              " 'essa',\n",
              " 'num',\n",
              " 'nem',\n",
              " 'suas',\n",
              " 'meu',\n",
              " 'às',\n",
              " 'minha',\n",
              " 'numa',\n",
              " 'pelos',\n",
              " 'elas',\n",
              " 'qual',\n",
              " 'nós',\n",
              " 'lhe',\n",
              " 'deles',\n",
              " 'essas',\n",
              " 'esses',\n",
              " 'pelas',\n",
              " 'este',\n",
              " 'dele',\n",
              " 'tu',\n",
              " 'te',\n",
              " 'vocês',\n",
              " 'vos',\n",
              " 'lhes',\n",
              " 'meus',\n",
              " 'minhas',\n",
              " 'teu',\n",
              " 'tua',\n",
              " 'teus',\n",
              " 'tuas',\n",
              " 'nosso',\n",
              " 'nossa',\n",
              " 'nossos',\n",
              " 'nossas',\n",
              " 'dela',\n",
              " 'delas',\n",
              " 'esta',\n",
              " 'estes',\n",
              " 'estas',\n",
              " 'aquele',\n",
              " 'aquela',\n",
              " 'aqueles',\n",
              " 'aquelas',\n",
              " 'isto',\n",
              " 'aquilo',\n",
              " 'estou',\n",
              " 'está',\n",
              " 'estamos',\n",
              " 'estão',\n",
              " 'estive',\n",
              " 'esteve',\n",
              " 'estivemos',\n",
              " 'estiveram',\n",
              " 'estava',\n",
              " 'estávamos',\n",
              " 'estavam',\n",
              " 'estivera',\n",
              " 'estivéramos',\n",
              " 'esteja',\n",
              " 'estejamos',\n",
              " 'estejam',\n",
              " 'estivesse',\n",
              " 'estivéssemos',\n",
              " 'estivessem',\n",
              " 'estiver',\n",
              " 'estivermos',\n",
              " 'estiverem',\n",
              " 'hei',\n",
              " 'há',\n",
              " 'havemos',\n",
              " 'hão',\n",
              " 'houve',\n",
              " 'houvemos',\n",
              " 'houveram',\n",
              " 'houvera',\n",
              " 'houvéramos',\n",
              " 'haja',\n",
              " 'hajamos',\n",
              " 'hajam',\n",
              " 'houvesse',\n",
              " 'houvéssemos',\n",
              " 'houvessem',\n",
              " 'houver',\n",
              " 'houvermos',\n",
              " 'houverem',\n",
              " 'houverei',\n",
              " 'houverá',\n",
              " 'houveremos',\n",
              " 'houverão',\n",
              " 'houveria',\n",
              " 'houveríamos',\n",
              " 'houveriam',\n",
              " 'sou',\n",
              " 'somos',\n",
              " 'são',\n",
              " 'era',\n",
              " 'éramos',\n",
              " 'eram',\n",
              " 'fui',\n",
              " 'foi',\n",
              " 'fomos',\n",
              " 'foram',\n",
              " 'fora',\n",
              " 'fôramos',\n",
              " 'seja',\n",
              " 'sejamos',\n",
              " 'sejam',\n",
              " 'fosse',\n",
              " 'fôssemos',\n",
              " 'fossem',\n",
              " 'for',\n",
              " 'formos',\n",
              " 'forem',\n",
              " 'serei',\n",
              " 'será',\n",
              " 'seremos',\n",
              " 'serão',\n",
              " 'seria',\n",
              " 'seríamos',\n",
              " 'seriam',\n",
              " 'tenho',\n",
              " 'tem',\n",
              " 'temos',\n",
              " 'tém',\n",
              " 'tinha',\n",
              " 'tínhamos',\n",
              " 'tinham',\n",
              " 'tive',\n",
              " 'teve',\n",
              " 'tivemos',\n",
              " 'tiveram',\n",
              " 'tivera',\n",
              " 'tivéramos',\n",
              " 'tenha',\n",
              " 'tenhamos',\n",
              " 'tenham',\n",
              " 'tivesse',\n",
              " 'tivéssemos',\n",
              " 'tivessem',\n",
              " 'tiver',\n",
              " 'tivermos',\n",
              " 'tiverem',\n",
              " 'terei',\n",
              " 'terá',\n",
              " 'teremos',\n",
              " 'terão',\n",
              " 'teria',\n",
              " 'teríamos',\n",
              " 'teriam']"
            ]
          },
          "execution_count": 26,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# stopwords.words('english')\n",
        "# stopwords.words('french')\n",
        "stopwords.words('portuguese')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGisszW6-It_",
        "outputId": "5c361236-f0ce-4654-f8b9-643c8bcc47a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "execution_count": 30,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(stopwords.words('english'))\n",
        "# len(stopwords.words('french'))\n",
        "# len(stopwords.words('portuguese'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D02PzdIRDSCr"
      },
      "outputs": [],
      "source": [
        "Filtering stopwords using REGEX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjMKMfgf-eb_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "punctuation=re.compile(r'[-.?!,:;()|0-9]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15ud-tH8B5SU",
        "outputId": "b1cc59d1-36cf-4ca3-9bca-9fa40998d614"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' ',\n",
              " ' ',\n",
              " 'A',\n",
              " 'b',\n",
              " 'a',\n",
              " 'l',\n",
              " 'a',\n",
              " 'h',\n",
              " 'a',\n",
              " 'm',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'I',\n",
              " 'z',\n",
              " 'a',\n",
              " 'k',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'I',\n",
              " 'z',\n",
              " 'a',\n",
              " 'k',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'a',\n",
              " 'k',\n",
              " 'o',\n",
              " 'b',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'a',\n",
              " 'k',\n",
              " 'o',\n",
              " 'b',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'u',\n",
              " 'd',\n",
              " 'a',\n",
              " ' ',\n",
              " 't',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'a',\n",
              " ' ',\n",
              " 'm',\n",
              " 'p',\n",
              " 'a',\n",
              " 'n',\n",
              " 'g',\n",
              " 'i',\n",
              " ' ',\n",
              " 'n',\n",
              " 'a',\n",
              " ' ',\n",
              " 'y',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'u',\n",
              " 'd',\n",
              " 'a',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'F',\n",
              " 'a',\n",
              " 'l',\n",
              " 'e',\n",
              " 'z',\n",
              " 'i',\n",
              " ' ',\n",
              " 't',\n",
              " 'i',\n",
              " ' ',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'l',\n",
              " 'a',\n",
              " ' ',\n",
              " 'y',\n",
              " 'a',\n",
              " ' ',\n",
              " 'T',\n",
              " 'a',\n",
              " 'm',\n",
              " 'a',\n",
              " 'r',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'F',\n",
              " 'a',\n",
              " 'l',\n",
              " 'e',\n",
              " 'z',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'E',\n",
              " 's',\n",
              " 'l',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'E',\n",
              " 's',\n",
              " 'l',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'A',\n",
              " 'l',\n",
              " 'a',\n",
              " 'm',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'A',\n",
              " 'l',\n",
              " 'a',\n",
              " 'm',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'A',\n",
              " 'm',\n",
              " 'i',\n",
              " 'n',\n",
              " 'a',\n",
              " 'd',\n",
              " 'a',\n",
              " 'b',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'A',\n",
              " 'm',\n",
              " 'i',\n",
              " 'n',\n",
              " 'a',\n",
              " 'd',\n",
              " 'a',\n",
              " 'b',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'N',\n",
              " 'a',\n",
              " 's',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'N',\n",
              " 'a',\n",
              " 's',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'S',\n",
              " 'a',\n",
              " 'l',\n",
              " 'm',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'S',\n",
              " 'a',\n",
              " 'l',\n",
              " 'm',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'B',\n",
              " 'u',\n",
              " 'z',\n",
              " 'i',\n",
              " ' ',\n",
              " 'n',\n",
              " 'a',\n",
              " ' ',\n",
              " 'L',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " 'b',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'B',\n",
              " 'u',\n",
              " 'z',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'O',\n",
              " 'b',\n",
              " 'e',\n",
              " 'd',\n",
              " 'i',\n",
              " ' ',\n",
              " 'n',\n",
              " 'a',\n",
              " ' ',\n",
              " 'L',\n",
              " 'u',\n",
              " 't',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'O',\n",
              " 'b',\n",
              " 'e',\n",
              " 'd',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'e',\n",
              " 's',\n",
              " 'e',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'e',\n",
              " 's',\n",
              " 'e',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'D',\n",
              " 'a',\n",
              " 'v',\n",
              " 'i',\n",
              " 'd',\n",
              " 'i',\n",
              " ' ',\n",
              " 'n',\n",
              " 't',\n",
              " 'o',\n",
              " 't',\n",
              " 'i',\n",
              " 'l',\n",
              " 'a',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'D',\n",
              " 'a',\n",
              " 'v',\n",
              " 'i',\n",
              " 'd',\n",
              " 'i',\n",
              " ' ',\n",
              " 'n',\n",
              " 't',\n",
              " 'o',\n",
              " 't',\n",
              " 'i',\n",
              " 'l',\n",
              " 'a',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'S',\n",
              " 'a',\n",
              " 'l',\n",
              " 'o',\n",
              " 'm',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " ' ',\n",
              " 'n',\n",
              " 'a',\n",
              " ' ',\n",
              " 'y',\n",
              " 'i',\n",
              " 'n',\n",
              " 'a',\n",
              " ' ',\n",
              " '[',\n",
              " 'y',\n",
              " 'a',\n",
              " ' ',\n",
              " 'v',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'n',\n",
              " 'k',\n",
              " 'e',\n",
              " 't',\n",
              " 'o',\n",
              " ']',\n",
              " ' ',\n",
              " 'y',\n",
              " 'a',\n",
              " ' ',\n",
              " 'U',\n",
              " 'l',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'S',\n",
              " 'a',\n",
              " 'l',\n",
              " 'o',\n",
              " 'm',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'L',\n",
              " 'o',\n",
              " 'b',\n",
              " 'o',\n",
              " 'a',\n",
              " 'm',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'L',\n",
              " 'o',\n",
              " 'b',\n",
              " 'o',\n",
              " 'a',\n",
              " 'm',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'A',\n",
              " 'b',\n",
              " 'i',\n",
              " 'a',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'A',\n",
              " 'b',\n",
              " 'i',\n",
              " 'a',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'A',\n",
              " 's',\n",
              " 'a',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'A',\n",
              " 's',\n",
              " 'a',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'o',\n",
              " 'z',\n",
              " 'a',\n",
              " 'f',\n",
              " 'a',\n",
              " 't',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'o',\n",
              " 'z',\n",
              " 'a',\n",
              " 'f',\n",
              " 'a',\n",
              " 't',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'o',\n",
              " 'l',\n",
              " 'a',\n",
              " 'm',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'o',\n",
              " 'l',\n",
              " 'a',\n",
              " 'm',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'O',\n",
              " 'z',\n",
              " 'i',\n",
              " 'a',\n",
              " 's',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'O',\n",
              " 'z',\n",
              " 'i',\n",
              " 'a',\n",
              " 's',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'o',\n",
              " 'a',\n",
              " 't',\n",
              " 'a',\n",
              " 'm',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'o',\n",
              " 'a',\n",
              " 't',\n",
              " 'a',\n",
              " 'm',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'A',\n",
              " 'k',\n",
              " 'a',\n",
              " 'z',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'A',\n",
              " 'k',\n",
              " 'a',\n",
              " 'z',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'E',\n",
              " 'z',\n",
              " 'e',\n",
              " 'k',\n",
              " 'i',\n",
              " 'a',\n",
              " 's',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'E',\n",
              " 'z',\n",
              " 'e',\n",
              " 'k',\n",
              " 'i',\n",
              " 'a',\n",
              " 's',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'M',\n",
              " 'a',\n",
              " 'n',\n",
              " 'a',\n",
              " 's',\n",
              " 'e',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'M',\n",
              " 'a',\n",
              " 'n',\n",
              " 'a',\n",
              " 's',\n",
              " 'e',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'A',\n",
              " 'm',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'A',\n",
              " 'm',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'o',\n",
              " 'z',\n",
              " 'i',\n",
              " 'a',\n",
              " 's',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'o',\n",
              " 'z',\n",
              " 'i',\n",
              " 'a',\n",
              " 's',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'e',\n",
              " 'k',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " 'a',\n",
              " 's',\n",
              " 'i',\n",
              " ' ',\n",
              " 't',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'a',\n",
              " ' ',\n",
              " 'm',\n",
              " 'p',\n",
              " 'a',\n",
              " 'n',\n",
              " 'g',\n",
              " 'i',\n",
              " ' ',\n",
              " 'n',\n",
              " 'a',\n",
              " ' ',\n",
              " 'y',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " 'i',\n",
              " ' ',\n",
              " 'n',\n",
              " 'a',\n",
              " ' ',\n",
              " 'n',\n",
              " 't',\n",
              " 'a',\n",
              " 'n',\n",
              " 'g',\n",
              " 'u',\n",
              " ' ',\n",
              " 'y',\n",
              " 'a',\n",
              " ' ',\n",
              " 'b',\n",
              " 'o',\n",
              " ' ',\n",
              " 'n',\n",
              " 'a',\n",
              " 't',\n",
              " 'a',\n",
              " 'm',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'n',\n",
              " 'a',\n",
              " ' ',\n",
              " 'B',\n",
              " 'a',\n",
              " 'b',\n",
              " 'i',\n",
              " 'l',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'n',\n",
              " 'a',\n",
              " ' ',\n",
              " 'n',\n",
              " 'i',\n",
              " 'm',\n",
              " 'a',\n",
              " ' ',\n",
              " 'y',\n",
              " 'a',\n",
              " ' ',\n",
              " 'k',\n",
              " 'u',\n",
              " 'n',\n",
              " 'a',\n",
              " 't',\n",
              " 'a',\n",
              " 'm',\n",
              " 'a',\n",
              " ' ',\n",
              " 'n',\n",
              " 'a',\n",
              " ' ',\n",
              " 'B',\n",
              " 'a',\n",
              " 'b',\n",
              " 'i',\n",
              " 'l',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " ' ',\n",
              " 'Y',\n",
              " 'e',\n",
              " 'k',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " 'a',\n",
              " 's',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'S',\n",
              " 'a',\n",
              " 'l',\n",
              " 'a',\n",
              " 't',\n",
              " 'i',\n",
              " 'e',\n",
              " 'l',\n",
              " 'e',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'S',\n",
              " 'a',\n",
              " 'l',\n",
              " 'a',\n",
              " 't',\n",
              " 'i',\n",
              " 'e',\n",
              " 'l',\n",
              " 'e',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'Z',\n",
              " 'o',\n",
              " 'l',\n",
              " 'o',\n",
              " 'b',\n",
              " 'a',\n",
              " 'b',\n",
              " 'e',\n",
              " 'l',\n",
              " 'e',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'Z',\n",
              " 'o',\n",
              " 'l',\n",
              " 'o',\n",
              " 'b',\n",
              " 'a',\n",
              " 'b',\n",
              " 'e',\n",
              " 'l',\n",
              " 'e',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'A',\n",
              " 'b',\n",
              " 'i',\n",
              " 'u',\n",
              " 'd',\n",
              " 'i',\n",
              " ' ',\n",
              " ' ',\n",
              " 'y',\n",
              " 'e',\n",
              " ' ',\n",
              " 'A',\n",
              " 'b',\n",
              " 'i',\n",
              " 'u',\n",
              " 'd',\n",
              " 'i',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " 'a',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'E',\n",
              " 'l',\n",
              " 'i',\n",
              " 'a',\n",
              " 'k',\n",
              " 'i',\n",
              " 'm',\n",
              " 'i',\n",
              " ' ',\n",
              " ...]"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "post_pontuation=[]\n",
        "for words in kikongo:\n",
        "  word=punctuation.sub('', words)\n",
        "  if len(word) > 0:\n",
        "    post_pontuation.append(word)\n",
        "\n",
        "post_pontuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfyxUeoQhW0h"
      },
      "source": [
        "## **POS Tag // POS Tagger**\n",
        "\n",
        "Traite une séquence de mots, et attache une partie de la balise vocale à chaque mot (n'oubliez pas d'importer nltk)\n",
        "\n",
        "Processes a **sequence of words**, and attaches a part of speech tag to each word (don't forget to import nltk): \n",
        "\n",
        "https://en.wikipedia.org/wiki/Part-of-speech_tagging\n",
        "\n",
        "https://www.researchgate.net/figure/Comparison-table-of-POS-tags_tbl1_327215090\n",
        "\n",
        "\n",
        "<!-- http://www.nltk.org/book/ch05.html -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQJU1tcTGkqp"
      },
      "outputs": [],
      "source": [
        "import nltk \n",
        "# from nltk.tokenize import averaged_perceptron_tagger\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = 'the world is a mess' # THE CORPUS HAS TO BE TOKENIZE FIRST!!!\n",
        "nltk.pos_tag(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTSE1fanF76U",
        "outputId": "3a831e7b-93d2-4a37-c8b4-34b107c4ac0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[('The', 'DT')]\n",
            "[('world', 'NN')]\n",
            "[('need', 'NN')]\n",
            "[('be', 'VB')]\n",
            "[('fixed', 'VBN')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "sent = \"The world need be fixed\"\n",
        "sent_tokens = word_tokenize(sent)\n",
        "for token in sent_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_ZYUE5QhXCV"
      },
      "source": [
        "## **Named Entity Recognition & Crunking** : Addictional Layer to POS tagging\n",
        "\n",
        "A process to recognize a *movie*, *monetary value*, *organization*, *location*, quantities, person.\n",
        "* Noun detection\n",
        "* phrase classification\n",
        "* entity disambiguation\n",
        "\n",
        "Google graph, IBM watson, wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN-yfTC6iaG5",
        "outputId": "ae95ba30-273e-46ab-9f84-21822753c28d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "(S\n",
            "  The/DT\n",
            "  (GSP US/NNP)\n",
            "  president/NN\n",
            "  stays/NNS\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (FACILITY White/NNP)\n",
            "  house/NN)\n"
          ]
        }
      ],
      "source": [
        "from nltk import ne_chunk\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "\n",
        "NE_sent = \"The US president stays in the White house\"\n",
        "\n",
        "NE_tokens = word_tokenize(NE_sent)\n",
        "NE_tags = nltk.pos_tag(NE_tokens)\n",
        "\n",
        "NE_NER = ne_chunk(NE_tags)\n",
        "print(NE_NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsWQ95eNhXPu"
      },
      "source": [
        "## ***SYNTAX TREE!!!!!***\n",
        "\n",
        "* SYNTAX : Rules + Principles + Process\n",
        "\n",
        "* SYNTAX TREE: is  a tree representation of syntactic structure of setences or strings\n",
        "* USEFUL for translation / phrase regenaration and rebuilding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7DoU_TcMEXT",
        "outputId": "74e0b201-7c0f-4714-a052-6d89dbe5ec56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ghostscripts (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for ghostscripts\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install ghostscripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKFNCgxAOdGi"
      },
      "source": [
        "USE JUPYTER and instal https://www.ghostscript.com/  !!!!\n",
        "\n",
        "To create the SYNTAX TREE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyLGB6iOPEgs"
      },
      "source": [
        "## **Crunking**\n",
        "\n",
        "Picking up Individual pieces of information and Grouping them into bigger PIECES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LluEMLD_V6o4",
        "outputId": "afe2318a-c447-4c2f-98b1-23499e18426a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('big', 'JJ'),\n",
              " ('cat', 'NN'),\n",
              " ('ate', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('little', 'JJ'),\n",
              " ('mouse', 'NN'),\n",
              " ('who', 'WP'),\n",
              " ('after', 'IN'),\n",
              " ('cheese', 'NN')]"
            ]
          },
          "execution_count": 52,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new = \"The big cat ate the little mouse who after cheese\"\n",
        "\n",
        "new_tokens = nltk.pos_tag(word_tokenize(new))\n",
        "new_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcbVnynRWbdG"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "\n",
        "import re\n",
        "\n",
        "grammar_np = r\"NP: {<DT>?<JJ>*<NN>}\" # NP : Noun Phrase : Phrase nominale!!!!(Sans verbs)\n",
        "\n",
        "chunk_parser = nltk.RegexpParser(grammar_np)\n",
        "\n",
        "chunk_result = chunk_parser.parse(new_tokens)\n",
        "\n",
        "chunk_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRTpsX7Tiawm"
      },
      "source": [
        "# **SEMANTIC ANALYSIS**\n",
        "\n",
        " * semantic wiki : https://en.wikipedia.org/wiki/Semantics\n",
        " * GenSim  : https://radimrehurek.com/gensim/index.html\n",
        " * Fasttext : https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py\n",
        " * Model2Vec : https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO0ytbTaingr"
      },
      "source": [
        "## Text / document classification\n",
        "\n",
        "### exemple from a **fastText** pre-trained model\n",
        "\n",
        "*design/create our own model ?!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLCtRQj4luvv",
        "outputId": "4e3068a8-3e0a-4166-ddc6-94955221fa60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENv21JtWlEfQ",
        "outputId": "f36a65b1-3f4f-4d75-9570-1406731407e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TECqBTzie1b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from gensim.models import FastText, Word2Vec, KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "WZ94vZbeSVrB",
        "outputId": "3739cdd5-3b7a-4142-a572-9be925fe5e7b"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bb026f2261d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/wiki.kg.vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'muntu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/wiki.kg.vec'"
          ]
        }
      ],
      "source": [
        "model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/wiki.kg.vec', binary=False, limit=10000)\n",
        "model.wv.most_similar('muntu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QMedGXPeo_4"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_fasttext():\n",
        "  print('loading word embeddings...')\n",
        "  embeddings_index = {}\n",
        "  f = open('/content/drive/MyDrive/Colab Notebooks/wiki.kg.vec', encoding='utf-8')\n",
        "  for line in tqdm(f):\n",
        "    values = line.strip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "  print('found %s word vectors' % len(embeddings_index)) \n",
        "  return embeddings_index\n",
        "\n",
        "embeddings_index=load_fasttext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5WVPQCf4pel"
      },
      "source": [
        "https://stackabuse.com/python-for-nlp-working-with-facebook-fasttext-library/\n",
        "https://medium.com/manash-en-blog/how-to-use-pre-trained-word-vectors-from-facebooks-fasttext-a71e6d55f27\n",
        "https://hackernoon.com/text-classification-simplified-with-facebooks-fasttext-b9d3022ac9cb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TzPqV7Ykkp7"
      },
      "source": [
        "## Text / document generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6aVNzCBieSX"
      },
      "source": [
        "## Machine Translation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1QXFeOd7j-gV",
        "pRTpsX7Tiawm"
      ],
      "name": "NLP-intro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b93d003b320fd3ebaa5319e95ea428037de38356353e5c5bf8dc2e7e7527d586"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
